{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77327ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipynb\n",
      "  Downloading ipynb-0.5.1-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: ipynb\n",
      "Successfully installed ipynb-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e626038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.phrases import Phraser\n",
    "# from ipynb.fs.full.preprocess import prepros, process\n",
    "# from preprocess import prepros, process\n",
    "# from SentimentLSTM import SentimentLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57ff42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_train = pd.read_csv('./input/full_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d09e582-d940-4e09-950e-83b9cd30450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process materials:\n",
    "ev_path = \"processors/Englishwords.xlsx\"\n",
    "sf_path =  \"processors/Shortform.xlsx\"\n",
    "stopwords_vn_path = \"processors/stopwords_vn_dash.txt\"\n",
    "englishwords = pd.read_excel(ev_path, index_col= \"English\")\n",
    "shortform = pd.read_excel(sf_path, index_col= \"Short\")\n",
    "\n",
    "#phraser for word2vec\n",
    "bigram = Phraser.load(\"saves/bigram.pkl\")\n",
    "\n",
    "#word2idx\n",
    "word2idx = pickle.load(open(\"saves/word2idx.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02be3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "def preprocess(text):\n",
    "  #bỏ tag html và emoji\n",
    "  text = re.sub('<[^>]*>', '', text)\n",
    "  text = deEmojify(text)\n",
    "\n",
    "  #thay chữ cái viết hoa thành viết thường\n",
    "  text = text.lower()\n",
    "\n",
    "  #xóa dấu ngắt câu, xóa link và các chữ có chứa chữ số\n",
    "  clean_text = []\n",
    "  punc_list = r'.,;:?!\\|/&@`~()-_@#$%^*\\'\\\"'\n",
    "  for w in (text.split()):\n",
    "    if \"http\" in w:\n",
    "      continue\n",
    "    clean_text.append(w)\n",
    "  text = ' '.join(clean_text)\n",
    "  for punc in punc_list:\n",
    "    text = text.replace(punc, ' ')\n",
    "\n",
    "  #xóa bỏ các chữ cái lặp liên tiếp nhau (đỉnhhhhhhhhhh, vipppppppppppppppp)\n",
    "  length = len(text)\n",
    "  char = 0\n",
    "  while char <length-1:\n",
    "    if text[char] == text[char+1]:\n",
    "      text = text[:char]+text[char+1:]\n",
    "      #print(text)\n",
    "      length-=1\n",
    "      continue\n",
    "    char+=1  \n",
    "  numbers = [\"không\", \"một\", \"hai\", \"ba\", \"bốn\", \"năm\", \"sáu\", \"bảy\", \"tám\", \"chín\"]\n",
    "  #chuyển đổi các từ tiếng anh và viết tắt thông dụng sang tiếng Việt chuẩn:\n",
    "  text_split = text.split()\n",
    "  for i, w in enumerate(text_split):\n",
    "    if w in englishwords.index:\n",
    "      text_split[i] = str(englishwords.loc[w, \"Vietnamese\"])\n",
    "    if w in shortform.index:\n",
    "      text_split[i] = str(shortform.loc[w, \"Long\"])\n",
    "    if w.isdigit():\n",
    "      text_split[i] = ' '.join([numbers[int(c)] for c in w]) \n",
    "  text = ' '.join(text_split)\n",
    "\n",
    "  #loại bỏ tất cả các kí tự đặc biệt còn lại\n",
    "  digits_and_characters = 'aăâbcdđeêfghijklmnoôơpqrstuưvxywzáàảãạắằẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789 '\n",
    "  text = ''.join([i for i in text if i in digits_and_characters])\n",
    "  return text\n",
    "#split all sentences in corpus\n",
    "def splitCorpus(corpus):\n",
    "  t = [sentence.split() for sentence in corpus]\n",
    "  return t\n",
    "#join all splited sentences to a big text document\n",
    "def joinAllSplit(tokenized_sentences):\n",
    "  sentences = [' '.join(sentence) for sentence in tokenized_sentences]\n",
    "  return ' '.join(sentences)\n",
    "\n",
    "#below function get performe preprocessing and remove unknown words\n",
    "def prepros(sentences):\n",
    "  new_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "  splitted_sentences = splitCorpus(new_sentences)\n",
    "  new = []\n",
    "  for sentence in bigram[splitted_sentences]:\n",
    "    new_sentence = ' '.join([word for word in sentence if word in word2idx.keys()])\n",
    "    new.append(new_sentence)\n",
    "  return new\n",
    "\n",
    "#convert words to numbers\n",
    "def sentenceToInt(sentences):\n",
    "  #print(sentences)\n",
    "  int_sentences = []\n",
    "  for sentence in sentences:\n",
    "    int_sentence = [word2idx[word] for word in sentence.split()]   \n",
    "    int_sentences.append(int_sentence)\n",
    "  return int_sentences\n",
    "\n",
    "#pad int_sentences to the feature_leng\n",
    "def padFeature(sentences, feature_leng = 50):\n",
    "  smatrix = np.zeros((len(sentences), feature_leng))\n",
    "  for sen_index, sentence in enumerate(sentences):\n",
    "    padding = max(0, feature_leng - len(sentence))\n",
    "    for word_index in range(feature_leng):\n",
    "      if word_index < padding:\n",
    "        smatrix[sen_index, word_index] = 0\n",
    "      else:\n",
    "        smatrix[sen_index, word_index] = sentence[word_index-padding]\n",
    "  return smatrix\n",
    "\n",
    "def process(sentences, feature_leng = 50):\n",
    "  int_sentences = sentenceToInt(sentences)\n",
    "  feature_matrix = padFeature(int_sentences, feature_leng = 50)\n",
    "  return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0881631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2idx\n",
    "word2idx = pickle.load(open(\"saves/word2idx.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0403d22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.4349e-02, 1.4035e-02, 9.0162e-01],\n",
      "        [1.9800e-07, 1.9296e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)\n",
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#model with 3 part: embedding layer -> stack lstms -> fc layers with softmax classifier\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, n_cell, emb_matrix, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze = False)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_dim, num_layers = n_layers, batch_first = True, dropout = drop_prob)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        #self.fc1 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        #self.relu1 = nn.LeakyReLU()\n",
    "        #self.fc2 = nn.Linear(hidden_dim*2, output_size)\n",
    "      \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        #print(x)\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.float()\n",
    "        #print(type(embeds))\n",
    "        #print(embeds)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        #print(out.shape)\n",
    "        #out = lstm_out[:, -1, :]\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        #out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        #print(out.shape)\n",
    "        # sigmoid function\n",
    "        #print(out.shape)\n",
    "        out = out.contiguous().view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.softmax(out)\n",
    "        # reshape to be batch_size first\n",
    "        #print(out.shape)\n",
    "        #out = out.view(batch_size,n_cell, -1)\n",
    "        #print(out.shape)\n",
    "        #out = out[:, -1] # get last batch of labels\n",
    "        #print(out.shape)\n",
    "        # return last sigmoid output and hidden state\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, train_on_gpu = False):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float())\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "comments = ['mon an nay rat dở', 'món ăn này rất ngon']\n",
    "pred, output, cleaned_comments = predict(model, comments= comments, sequence_length= 50, train_on_gpu= False)\n",
    "\n",
    "list_pred = pred.tolist()\n",
    "print(list_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206534da-782a-4ea0-91eb-c3fae95e05e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b51a1c8b3abad5016470c0e3207cfc7c8c3ca419353ce686821d3b9cf6749287"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
