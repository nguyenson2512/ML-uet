{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63271420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#model with 3 part: embedding layer -> stack lstms -> fc layers with softmax classifier\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, n_cell, emb_matrix, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze = False)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_dim, num_layers = n_layers, batch_first = True, dropout = drop_prob)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        #self.fc1 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        #self.relu1 = nn.LeakyReLU()\n",
    "        #self.fc2 = nn.Linear(hidden_dim*2, output_size)\n",
    "      \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        #print(x)\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.float()\n",
    "        #print(type(embeds))\n",
    "        #print(embeds)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        #print(out.shape)\n",
    "        #out = lstm_out[:, -1, :]\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        #out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        #print(out.shape)\n",
    "        # sigmoid function\n",
    "        #print(out.shape)\n",
    "        out = out.contiguous().view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.softmax(out)\n",
    "        # reshape to be batch_size first\n",
    "        #print(out.shape)\n",
    "        #out = out.view(batch_size,n_cell, -1)\n",
    "        #print(out.shape)\n",
    "        #out = out[:, -1] # get last batch of labels\n",
    "        #print(out.shape)\n",
    "        # return last sigmoid output and hidden state\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, train_on_gpu = False):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd48da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
