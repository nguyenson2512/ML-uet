{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278e8447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.20.8\n",
      "  Downloading Levenshtein-0.20.8-cp39-cp39-win_amd64.whl (100 kB)\n",
      "     -------------------------------------- 100.6/100.6 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.3-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.20.8 python-Levenshtein-0.20.8 rapidfuzz-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2d0bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.phrases import Phraser\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe9bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process materials:\n",
    "ev_path = \"processors/Englishwords.xlsx\"\n",
    "sf_path =  \"processors/Shortform.xlsx\"\n",
    "stopwords_vn_path = \"processors/stopwords_vn_dash.txt\"\n",
    "englishwords = pd.read_excel(ev_path, index_col= \"English\")\n",
    "shortform = pd.read_excel(sf_path, index_col= \"Short\")\n",
    "\n",
    "#phraser for word2vec\n",
    "bigram = Phraser.load(\"saves/bigram.pkl\")\n",
    "\n",
    "#word2idx\n",
    "word2idx = pickle.load(open(\"saves/word2idx.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff975f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Vietnamese\n",
      "English                   \n",
      "access            truy cập\n",
      "adapter            cục sạc\n",
      "ah                       à\n",
      "ak                       à\n",
      "app               ứng dụng\n",
      "...                    ...\n",
      "try           thử/ cố gắng\n",
      "website          trang web\n",
      "wireless         không dây\n",
      "workshop  buổi diễn thuyết\n",
      "wow                      ồ\n",
      "\n",
      "[66 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(englishwords)\n",
    "# print(bigram)\n",
    "# print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb9bf417-784e-4fd3-a4aa-36ce9f2d2808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a1e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65064e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string\n",
    "def preprocess(text):  \n",
    "  try:\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = deEmojify(text)\n",
    "  except:\n",
    "    print(\"An exception occurred\", text)\n",
    "  \n",
    "\n",
    "  #thay chữ cái viết hoa thành viết thường\n",
    "  text = text.lower()\n",
    "\n",
    "  #xóa dấu ngắt câu, xóa link và các chữ có chứa chữ số\n",
    "  clean_text = []\n",
    "  punc_list = r'.,;:?!\\|/&@`~()-_@#$%^*\\'\\\"'\n",
    "  for w in (text.split()):\n",
    "    if \"http\" in w:\n",
    "      continue\n",
    "    clean_text.append(w)\n",
    "  text = ' '.join(clean_text)\n",
    "  for punc in punc_list:\n",
    "    text = text.replace(punc, ' ')\n",
    "\n",
    "  #xóa bỏ các chữ cái lặp liên tiếp nhau (đỉnhhhhhhhhhh, vipppppppppppppppp)\n",
    "  length = len(text)\n",
    "  char = 0\n",
    "  while char <length-1:\n",
    "    if text[char] == text[char+1]:\n",
    "      text = text[:char]+text[char+1:]\n",
    "      #print(text)\n",
    "      length-=1\n",
    "      continue\n",
    "    char+=1  \n",
    "  numbers = [\"không\", \"một\", \"hai\", \"ba\", \"bốn\", \"năm\", \"sáu\", \"bảy\", \"tám\", \"chín\"]\n",
    "  #chuyển đổi các từ tiếng anh và viết tắt thông dụng sang tiếng Việt chuẩn:\n",
    "  text_split = text.split()\n",
    "  for i, w in enumerate(text_split):\n",
    "    if w in englishwords.index:\n",
    "      text_split[i] = str(englishwords.loc[w, \"Vietnamese\"])\n",
    "    if w in shortform.index:\n",
    "      text_split[i] = str(shortform.loc[w, \"Long\"])\n",
    "    if w.isdigit():\n",
    "      text_split[i] = ' '.join([numbers[int(c)] for c in w]) \n",
    "  text = ' '.join(text_split)\n",
    "\n",
    "  #loại bỏ tất cả các kí tự đặc biệt còn lại\n",
    "  digits_and_characters = 'aăâbcdđeêfghijklmnoôơpqrstuưvxywzáàảãạắằẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789 '\n",
    "  text = ''.join([i for i in text if i in digits_and_characters])\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a13842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a giờ ă âbcdđ giờ\n"
     ]
    }
   ],
   "source": [
    "x = preprocess('aaa h ă âbcdđ h')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15031457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split all sentences in corpus\n",
    "def splitCorpus(corpus):\n",
    "  t = [sentence.split() for sentence in corpus]\n",
    "  return t\n",
    "#join all splited sentences to a big text document\n",
    "def joinAllSplit(tokenized_sentences):\n",
    "  sentences = [' '.join(sentence) for sentence in tokenized_sentences]\n",
    "  return ' '.join(sentences)\n",
    "\n",
    "#below function get performe preprocessing and remove unknown words\n",
    "def prepros(sentences):\n",
    "  new_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "  splitted_sentences = splitCorpus(new_sentences)\n",
    "  new = []\n",
    "  for sentence in bigram[splitted_sentences]:\n",
    "    new_sentence = ' '.join([word for word in sentence if word in word2idx.keys()])\n",
    "    new.append(new_sentence)\n",
    "  return new\n",
    "\n",
    "#convert words to numbers\n",
    "def sentenceToInt(sentences):\n",
    "  #print(sentences)\n",
    "  int_sentences = []\n",
    "  for sentence in sentences:\n",
    "    int_sentence = [word2idx[word] for word in sentence.split()]   \n",
    "    int_sentences.append(int_sentence)\n",
    "  return int_sentences\n",
    "\n",
    "#pad int_sentences to the feature_leng\n",
    "def padFeature(sentences, feature_leng = 50):\n",
    "  smatrix = np.zeros((len(sentences), feature_leng))\n",
    "  for sen_index, sentence in enumerate(sentences):\n",
    "    padding = max(0, feature_leng - len(sentence))\n",
    "    for word_index in range(feature_leng):\n",
    "      if word_index < padding:\n",
    "        smatrix[sen_index, word_index] = 0\n",
    "      else:\n",
    "        smatrix[sen_index, word_index] = sentence[word_index-padding]\n",
    "  return smatrix\n",
    "\n",
    "def process(sentences, feature_leng = 50):\n",
    "  int_sentences = sentenceToInt(sentences)\n",
    "  feature_matrix = padFeature(int_sentences, feature_leng = 50)\n",
    "  return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313db933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#model with 3 part: embedding layer -> stack lstms -> fc layers with softmax classifier\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, n_cell, emb_matrix, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze = False)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_dim, num_layers = n_layers, batch_first = True, dropout = drop_prob)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        #self.fc1 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        #self.relu1 = nn.LeakyReLU()\n",
    "        #self.fc2 = nn.Linear(hidden_dim*2, output_size)\n",
    "      \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        #print(x)\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.float()\n",
    "        #print(type(embeds))\n",
    "        #print(embeds)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        #print(out.shape)\n",
    "        #out = lstm_out[:, -1, :]\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        #out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        #print(out.shape)\n",
    "        # sigmoid function\n",
    "        #print(out.shape)\n",
    "        out = out.contiguous().view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.softmax(out)\n",
    "        # reshape to be batch_size first\n",
    "        #print(out.shape)\n",
    "        #out = out.view(batch_size,n_cell, -1)\n",
    "        #print(out.shape)\n",
    "        #out = out[:, -1] # get last batch of labels\n",
    "        #print(out.shape)\n",
    "        # return last sigmoid output and hidden state\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, train_on_gpu = False):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f82667e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thời tiết lạnh như này, cả nhà rủ nhau đến leg...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Em có đọc review thấy mng bảo trà sữa nướng đề...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Rating\n",
       "0  Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...     1.0\n",
       "1  Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...     0.0\n",
       "2  Thời tiết lạnh như này, cả nhà rủ nhau đến leg...     1.0\n",
       "3  Em có đọc review thấy mng bảo trà sữa nướng đề...     0.0\n",
       "4  Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...     1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "train = pd.read_csv(\"./input/full_train.csv\")\n",
    "\n",
    "# drop columns 'RevId','UserId','image_urls', 'Unnamed: 0'\n",
    "train.drop(columns=['RevId','UserId','image_urls', 'Unnamed: 0'], inplace=True)\n",
    "test.drop(columns=['RevId','UserId','image_urls', 'Unnamed: 0'], inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67381bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data into train and validation \n",
    "# train_df, valid_df = train_test_split(train)\n",
    "# print(train_df.shape)\n",
    "# print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b3bf2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9070, 2)\n",
      "(9070, 2)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with empty text\n",
    "print(train.shape)\n",
    "train = train[train.Comment == train.Comment]\n",
    "train = train[train.Rating == train.Rating]\n",
    "print(train.shape)\n",
    "\n",
    "X,y = train['Comment'].values,train['Rating'].values\n",
    "# X = prepros(X)\n",
    "# print(X)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6960eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array([1.0,0.0]),y=dd.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19dcdc85-37f0-4d9b-a8ff-0d717b9d2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8093.  4946.  1071. ...  4600.  9516.  5907.]\n",
      " [    0.     0.     0. ...  1934. 10645.   477.]\n",
      " [ 2815.  9311.  2564. ...   542.  7067. 10463.]\n",
      " ...\n",
      " [    0.     0.     0. ... 10978. 11646.  6661.]\n",
      " [    0.     0.     0. ...  9419. 10707. 10721.]\n",
      " [    0.     0.     0. ...   943. 12219.  4262.]]\n",
      "====\n",
      "[[    0.     0.     0. ...  7343.  7596. 10389.]\n",
      " [ 3139.  5151. 10707. ... 10352.  9209.  4220.]\n",
      " [    0.     0.     0. ... 10721.  4551. 11146.]\n",
      " ...\n",
      " [   86.  9311.  4551. ...  8173.  1113.  6147.]\n",
      " [ 9068. 11730.  5236. ...  9419.   542.  4220.]\n",
      " [10707.  3139.  2097. ...  9516.  6958.  8360.]]\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "x_train = prepros(x_train)\n",
    "x_train = process(x_train)\n",
    "print(x_train)\n",
    "print('====')\n",
    "x_test = prepros(x_test)\n",
    "x_test = process(x_test)\n",
    "print(x_test)\n",
    "print('====')\n",
    "# print(y_train)\n",
    "# print('====')\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d420eba-9e6b-4921-b607-028ee391a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x000001A7C297AF70>\n"
     ]
    }
   ],
   "source": [
    "# create Tensor datasets\n",
    "\n",
    "\n",
    "x_train = x_train.astype(np.int64)\n",
    "y_train = y_train.astype(np.int64)\n",
    "x_test = x_test.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "print(train_data)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "# for i, (inputs, targets) in enumerate(train_loader):\n",
    "#     with torch.no_grad():\n",
    "#         inputs = Variable(inputs)\n",
    "#         targets = Variable(targets)\n",
    "#         print(\"targets.data\", targets)\n",
    "#         print(\"inputs.data\", inputs)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c9df846-cbd8-4dd7-bd6b-1b44c78c88da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001A7C77D25B0>\n",
      "<torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x000001A7B5364940>\n",
      "Sample input size:  torch.Size([50, 50])\n",
      "Sample input: \n",
      " tensor([[ 1677,  1071, 12304,  ...,   565,  3275,  8056],\n",
      "        [ 6147, 11608,  2815,  ...,  9717,  4696,  3640],\n",
      "        [    0,     0,     0,  ...,  5236,  5665,  3086],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 11730,  7225,  4391],\n",
      "        [ 9491,  5151,  2564,  ..., 10319,  9516,  4448],\n",
      "        [11455,  6321,  3032,  ...,  6127, 10821,  6574]])\n",
      "Sample input: \n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "print(train_loader)\n",
    "dataiter = iter(train_loader)\n",
    "print(dataiter)\n",
    "# sample_x, sample_y = dataiter.next()\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample input: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8ea13227-2411-4642-ba12-21595bf233cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(12345, 200)\n",
      "  (lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# #word2idx\n",
    "# word2idx = pickle.load(open(\"saves/word2idx.pickle\", \"rb\"))\n",
    "\n",
    "# # Load model\n",
    "# emb_matrix = torch.zeros((len(word2idx)+1,200))\n",
    "\n",
    "# model = SentimentLSTM(output_size= 3, embedding_dim=200, hidden_dim= 128, n_layers= 2, n_cell = 50, emb_matrix=emb_matrix, drop_prob = 0.2)\n",
    "\n",
    "# #moving to gpu\n",
    "# # model.to(device)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e136fdd0-31f0-45ed-941f-cbc193af2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden\n",
    "\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "090d6bac-b5b9-40d3-bafa-8abec491eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(12345, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "# vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "# print(vocab_size)\n",
    "vocab_size = len(word2idx)+1\n",
    "\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca93b279-c9d6-4ce3-8cc6-6235a5205bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72dd978d-f499-4bbe-82d8-f6ea376de1b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 2, 256), got [2, 50, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31532\\2073135160.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# calculate the loss and perform backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31532\\4238004862.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# shape: B x S x Feature   since batch = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#print(embeds.shape)  #[50, 500, 1000]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mlstm_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    696\u001b[0m                            ):\n\u001b[0;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[0;32m    699\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[0;32m    700\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    229\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 2, 256), got [2, 50, 256]"
     ]
    }
   ],
   "source": [
    "clip = 5\n",
    "epochs = 5 \n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), './working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eacbdc-ef03-47a7-960c-8c28a6fe46d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
