{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278e8447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.20.8\n",
      "  Downloading Levenshtein-0.20.8-cp38-cp38-macosx_10_9_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 710 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.3-cp38-cp38-macosx_10_9_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.20.8 python-Levenshtein-0.20.8 rapidfuzz-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d0bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe9bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process materials:\n",
    "ev_path = \"processors/Englishwords.xlsx\"\n",
    "sf_path =  \"processors/Shortform.xlsx\"\n",
    "stopwords_vn_path = \"processors/stopwords_vn_dash.txt\"\n",
    "englishwords = pd.read_excel(ev_path, index_col= \"English\")\n",
    "shortform = pd.read_excel(sf_path, index_col= \"Short\")\n",
    "\n",
    "#phraser for word2vec\n",
    "bigram = Phraser.load(\"saves/bigram.pkl\")\n",
    "\n",
    "#word2idx\n",
    "word2idx = pickle.load(open(\"saves/word2idx.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff975f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Vietnamese\n",
      "English                   \n",
      "access            truy cập\n",
      "adapter            cục sạc\n",
      "ah                       à\n",
      "ak                       à\n",
      "app               ứng dụng\n",
      "...                    ...\n",
      "try           thử/ cố gắng\n",
      "website          trang web\n",
      "wireless         không dây\n",
      "workshop  buổi diễn thuyết\n",
      "wow                      ồ\n",
      "\n",
      "[66 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(englishwords)\n",
    "# print(bigram)\n",
    "# print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85a1e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65064e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string\n",
    "def preprocess(text):  \n",
    "  if isNaN(text):\n",
    "    text = ''\n",
    "    print('true')\n",
    "    return text\n",
    "  try:\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = deEmojify(text)\n",
    "  except:\n",
    "    print(\"An exception occurred\", text)\n",
    "  \n",
    "\n",
    "  #thay chữ cái viết hoa thành viết thường\n",
    "  text = text.lower()\n",
    "\n",
    "  #xóa dấu ngắt câu, xóa link và các chữ có chứa chữ số\n",
    "  clean_text = []\n",
    "  punc_list = r'.,;:?!\\|/&@`~()-_@#$%^*\\'\\\"'\n",
    "  for w in (text.split()):\n",
    "    if \"http\" in w:\n",
    "      continue\n",
    "    clean_text.append(w)\n",
    "  text = ' '.join(clean_text)\n",
    "  for punc in punc_list:\n",
    "    text = text.replace(punc, ' ')\n",
    "\n",
    "  #xóa bỏ các chữ cái lặp liên tiếp nhau (đỉnhhhhhhhhhh, vipppppppppppppppp)\n",
    "  length = len(text)\n",
    "  char = 0\n",
    "  while char <length-1:\n",
    "    if text[char] == text[char+1]:\n",
    "      text = text[:char]+text[char+1:]\n",
    "      #print(text)\n",
    "      length-=1\n",
    "      continue\n",
    "    char+=1  \n",
    "  numbers = [\"không\", \"một\", \"hai\", \"ba\", \"bốn\", \"năm\", \"sáu\", \"bảy\", \"tám\", \"chín\"]\n",
    "  #chuyển đổi các từ tiếng anh và viết tắt thông dụng sang tiếng Việt chuẩn:\n",
    "  text_split = text.split()\n",
    "  for i, w in enumerate(text_split):\n",
    "    if w in englishwords.index:\n",
    "      text_split[i] = str(englishwords.loc[w, \"Vietnamese\"])\n",
    "    if w in shortform.index:\n",
    "      text_split[i] = str(shortform.loc[w, \"Long\"])\n",
    "    if w.isdigit():\n",
    "      text_split[i] = ' '.join([numbers[int(c)] for c in w]) \n",
    "  text = ' '.join(text_split)\n",
    "\n",
    "  #loại bỏ tất cả các kí tự đặc biệt còn lại\n",
    "  digits_and_characters = 'aăâbcdđeêfghijklmnoôơpqrstuưvxywzáàảãạắằẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789 '\n",
    "  text = ''.join([i for i in text if i in digits_and_characters])\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b7a13842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a giờ ă âbcdđ giờ\n"
     ]
    }
   ],
   "source": [
    "x = preprocess('aaa h ă âbcdđ h')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "15031457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split all sentences in corpus\n",
    "def splitCorpus(corpus):\n",
    "  t = [sentence.split() for sentence in corpus]\n",
    "  return t\n",
    "#join all splited sentences to a big text document\n",
    "def joinAllSplit(tokenized_sentences):\n",
    "  sentences = [' '.join(sentence) for sentence in tokenized_sentences]\n",
    "  return ' '.join(sentences)\n",
    "\n",
    "#below function get performe preprocessing and remove unknown words\n",
    "def prepros(sentences):\n",
    "  new_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "  splitted_sentences = splitCorpus(new_sentences)\n",
    "  new = []\n",
    "  for sentence in bigram[splitted_sentences]:\n",
    "    new_sentence = ' '.join([word for word in sentence if word in word2idx.keys()])\n",
    "    new.append(new_sentence)\n",
    "  return new\n",
    "\n",
    "#convert words to numbers\n",
    "def sentenceToInt(sentences):\n",
    "  #print(sentences)\n",
    "  int_sentences = []\n",
    "  for sentence in sentences:\n",
    "    int_sentence = [word2idx[word] for word in sentence.split()]   \n",
    "    int_sentences.append(int_sentence)\n",
    "  return int_sentences\n",
    "\n",
    "#pad int_sentences to the feature_leng\n",
    "def padFeature(sentences, feature_leng = 50):\n",
    "  smatrix = np.zeros((len(sentences), feature_leng))\n",
    "  for sen_index, sentence in enumerate(sentences):\n",
    "    padding = max(0, feature_leng - len(sentence))\n",
    "    for word_index in range(feature_leng):\n",
    "      if word_index < padding:\n",
    "        smatrix[sen_index, word_index] = 0\n",
    "      else:\n",
    "        smatrix[sen_index, word_index] = sentence[word_index-padding]\n",
    "  return smatrix\n",
    "\n",
    "def process(sentences, feature_leng = 50):\n",
    "  int_sentences = sentenceToInt(sentences)\n",
    "  feature_matrix = padFeature(int_sentences, feature_leng = 50)\n",
    "  return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "313db933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#model with 3 part: embedding layer -> stack lstms -> fc layers with softmax classifier\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, n_cell, emb_matrix, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze = False)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_dim, num_layers = n_layers, batch_first = True, dropout = drop_prob)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        #self.fc1 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        #self.relu1 = nn.LeakyReLU()\n",
    "        #self.fc2 = nn.Linear(hidden_dim*2, output_size)\n",
    "      \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        #print(x)\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.float()\n",
    "        #print(type(embeds))\n",
    "        #print(embeds)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        #print(out.shape)\n",
    "        #out = lstm_out[:, -1, :]\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        #out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        #print(out.shape)\n",
    "        # sigmoid function\n",
    "        #print(out.shape)\n",
    "        out = out.contiguous().view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.softmax(out)\n",
    "        # reshape to be batch_size first\n",
    "        #print(out.shape)\n",
    "        #out = out.view(batch_size,n_cell, -1)\n",
    "        #print(out.shape)\n",
    "        #out = out[:, -1] # get last batch of labels\n",
    "        #print(out.shape)\n",
    "        # return last sigmoid output and hidden state\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, train_on_gpu = False):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f82667e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thời tiết lạnh như này, cả nhà rủ nhau đến leg...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Em có đọc review thấy mng bảo trà sữa nướng đề...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Rating\n",
       "0  Xôi dẻo, đồ ăn đậm vị. Hộp xôi được lót lá trô...     1.0\n",
       "1  Gọi ship 1 xuất cari gà bánh naan và 3 miếng g...     0.0\n",
       "2  Thời tiết lạnh như này, cả nhà rủ nhau đến leg...     1.0\n",
       "3  Em có đọc review thấy mng bảo trà sữa nướng đề...     0.0\n",
       "4  Đồ ăn rất ngon, nhà hàng cũng rất đẹp, tất cả ...     1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "test = pd.read_csv(\"./input/test.csv\")\n",
    "train = pd.read_csv(\"./input/full_train.csv\")\n",
    "\n",
    "# drop columns 'RevId','UserId','image_urls', 'Unnamed: 0'\n",
    "train.drop(columns=['RevId','UserId','image_urls', 'Unnamed: 0'], inplace=True)\n",
    "test.drop(columns=['RevId','UserId','image_urls', 'Unnamed: 0'], inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67381bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6804, 2)\n",
      "(2269, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data into train and validation \n",
    "# train_df, valid_df = train_test_split(train)\n",
    "# print(train_df.shape)\n",
    "# print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b3bf2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(0, 2)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with empty text\n",
    "print(train.shape)\n",
    "train = train[train.Comment == train.Comment]\n",
    "print(train.shape)\n",
    "\n",
    "# X,y = train['Comment'].values,train['Rating'].values\n",
    "# X = prepros(X)\n",
    "# print(X)\n",
    "# x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "\n",
    "\n",
    "# print([X[0],X[1], X[2]])\n",
    "# print([y[0], y[1], y[2]])\n",
    "# print(y.dtypes)\n",
    "# z = np.array([y[0], y[1], y[2]], dtype=float)\n",
    "# x_train,x_test,y_train,y_test = train_test_split([X[0],X[1], X[2]],z,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6960eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.Series(y_train).value_counts()\n",
    "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
