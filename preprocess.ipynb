{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42cc49f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle-mixin\n",
      "  Downloading pickle-mixin-1.0.2.tar.gz (5.1 kB)\n",
      "Building wheels for collected packages: pickle-mixin\n",
      "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-py3-none-any.whl size=5998 sha256=5367dfd88371776c397bff2c1ea6f0cd4a6aef02ecfaa8b3b47f936aba8e516c\n",
      "  Stored in directory: /Users/sonnguyen2k/Library/Caches/pip/wheels/2a/a4/6c/83bfbc3b94f1bb43d634b07a6a893fd437a45c58b29aea5142\n",
      "Successfully built pickle-mixin\n",
      "Installing collected packages: pickle-mixin\n",
      "Successfully installed pickle-mixin-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c603d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5369a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/sonnguyen2k/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - gensim\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.12.0               |   py38hecd8cb5_0        14.5 MB\n",
      "    gensim-4.0.1               |   py38h23ab428_0        18.2 MB\n",
      "    smart_open-5.2.1           |   py38hecd8cb5_0          77 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        32.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  gensim             pkgs/main/osx-64::gensim-4.0.1-py38h23ab428_0\n",
      "  smart_open         pkgs/main/osx-64::smart_open-5.2.1-py38hecd8cb5_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                               4.10.3-py38hecd8cb5_0 --> 4.12.0-py38hecd8cb5_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-4.12.0         | 14.5 MB   | ##################################### | 100% \n",
      "smart_open-5.2.1     | 77 KB     | ##################################### | 100% \n",
      "gensim-4.0.1         | 18.2 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "import sys\n",
    "!conda install --yes gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90514082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38eb0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process materials:\n",
    "ev_path = \"processors/Englishwords.xlsx\"\n",
    "sf_path =  \"processors/Shortform.xlsx\"\n",
    "stopwords_vn_path = \"processors/stopwords_vn_dash.txt\"\n",
    "englishwords = pd.read_excel(ev_path, index_col= \"English\")\n",
    "shortform = pd.read_excel(sf_path, index_col= \"Short\")\n",
    "\n",
    "#phraser for word2vec\n",
    "bigram = Phraser.load(\"saves/bigram.pkl\")\n",
    "\n",
    "#word2idx\n",
    "word2idx = pickle.load(open(\"saves/word2idx.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade5098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Vietnamese\n",
      "English                   \n",
      "access            truy cập\n",
      "adapter            cục sạc\n",
      "ah                       à\n",
      "ak                       à\n",
      "app               ứng dụng\n",
      "...                    ...\n",
      "try           thử/ cố gắng\n",
      "website          trang web\n",
      "wireless         không dây\n",
      "workshop  buổi diễn thuyết\n",
      "wow                      ồ\n",
      "\n",
      "[66 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(englishwords)\n",
    "# print(bigram)\n",
    "# print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "461c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195fe0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  #bỏ tag html và emoji\n",
    "  text = re.sub('<[^>]*>', '', text)\n",
    "  text = deEmojify(text)\n",
    "\n",
    "  #thay chữ cái viết hoa thành viết thường\n",
    "  text = text.lower()\n",
    "\n",
    "  #xóa dấu ngắt câu, xóa link và các chữ có chứa chữ số\n",
    "  clean_text = []\n",
    "  punc_list = r'.,;:?!\\|/&@`~()-_@#$%^*\\'\\\"'\n",
    "  for w in (text.split()):\n",
    "    if \"http\" in w:\n",
    "      continue\n",
    "    clean_text.append(w)\n",
    "  text = ' '.join(clean_text)\n",
    "  for punc in punc_list:\n",
    "    text = text.replace(punc, ' ')\n",
    "\n",
    "  #xóa bỏ các chữ cái lặp liên tiếp nhau (đỉnhhhhhhhhhh, vipppppppppppppppp)\n",
    "  length = len(text)\n",
    "  char = 0\n",
    "  while char <length-1:\n",
    "    if text[char] == text[char+1]:\n",
    "      text = text[:char]+text[char+1:]\n",
    "      #print(text)\n",
    "      length-=1\n",
    "      continue\n",
    "    char+=1  \n",
    "  numbers = [\"không\", \"một\", \"hai\", \"ba\", \"bốn\", \"năm\", \"sáu\", \"bảy\", \"tám\", \"chín\"]\n",
    "  #chuyển đổi các từ tiếng anh và viết tắt thông dụng sang tiếng Việt chuẩn:\n",
    "  text_split = text.split()\n",
    "  for i, w in enumerate(text_split):\n",
    "    if w in englishwords.index:\n",
    "      text_split[i] = str(englishwords.loc[w, \"Vietnamese\"])\n",
    "    if w in shortform.index:\n",
    "      text_split[i] = str(shortform.loc[w, \"Long\"])\n",
    "    if w.isdigit():\n",
    "      text_split[i] = ' '.join([numbers[int(c)] for c in w]) \n",
    "  text = ' '.join(text_split)\n",
    "\n",
    "  #loại bỏ tất cả các kí tự đặc biệt còn lại\n",
    "  digits_and_characters = 'aăâbcdđeêfghijklmnoôơpqrstuưvxywzáàảãạắằẳẵặấầẩẫậéèẻẽẹếềểễệíìỉĩịóòỏõọốồổỗộớờởỡợúùủũụứừửữựýỳỷỹỵ0123456789 '\n",
    "  text = ''.join([i for i in text if i in digits_and_characters])\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60294528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phục vụ nhân viên lễ phép nhiệt tình\n"
     ]
    }
   ],
   "source": [
    "x = preprocess('Phục vụ: nhân viên lễ phép nhiệt tình')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b64d17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split all sentences in corpus\n",
    "def splitCorpus(corpus):\n",
    "  t = [sentence.split() for sentence in corpus]\n",
    "  return t\n",
    "#join all splited sentences to a big text document\n",
    "def joinAllSplit(tokenized_sentences):\n",
    "  sentences = [' '.join(sentence) for sentence in tokenized_sentences]\n",
    "  return ' '.join(sentences)\n",
    "\n",
    "#below function get performe preprocessing and remove unknown words\n",
    "def prepros(sentences):\n",
    "  new_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "  splitted_sentences = splitCorpus(new_sentences)\n",
    "  new = []\n",
    "  for sentence in bigram[splitted_sentences]:\n",
    "    new_sentence = ' '.join([word for word in sentence if word in word2idx.keys()])\n",
    "    new.append(new_sentence)\n",
    "  return new\n",
    "\n",
    "#convert words to numbers\n",
    "def sentenceToInt(sentences):\n",
    "  #print(sentences)\n",
    "  int_sentences = []\n",
    "  for sentence in sentences:\n",
    "    int_sentence = [word2idx[word] for word in sentence.split()]   \n",
    "    int_sentences.append(int_sentence)\n",
    "  return int_sentences\n",
    "\n",
    "#pad int_sentences to the feature_leng\n",
    "def padFeature(sentences, feature_leng = 50):\n",
    "  smatrix = np.zeros((len(sentences), feature_leng))\n",
    "  for sen_index, sentence in enumerate(sentences):\n",
    "    padding = max(0, feature_leng - len(sentence))\n",
    "    for word_index in range(feature_leng):\n",
    "      if word_index < padding:\n",
    "        smatrix[sen_index, word_index] = 0\n",
    "      else:\n",
    "        smatrix[sen_index, word_index] = sentence[word_index-padding]\n",
    "  return smatrix\n",
    "\n",
    "def process(sentences, feature_leng = 50):\n",
    "  int_sentences = sentenceToInt(sentences)\n",
    "  feature_matrix = padFeature(int_sentences, feature_leng = 50)\n",
    "  return feature_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc908fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#model with 3 part: embedding layer -> stack lstms -> fc layers with softmax classifier\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, n_cell, emb_matrix, drop_prob = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(emb_matrix, freeze = False)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,hidden_size = hidden_dim, num_layers = n_layers, batch_first = True, dropout = drop_prob)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        #self.fc1 = nn.Linear(hidden_dim, hidden_dim*2)\n",
    "        #self.relu1 = nn.LeakyReLU()\n",
    "        #self.fc2 = nn.Linear(hidden_dim*2, output_size)\n",
    "      \n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        #print(x)\n",
    "        # embeddings and lstm_out\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.float()\n",
    "        #print(type(embeds))\n",
    "        #print(embeds)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        #print(lstm_out.shape)\n",
    "        #stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        #print(out.shape)\n",
    "        #out = lstm_out[:, -1, :]\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        #out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        #print(out.shape)\n",
    "        # sigmoid function\n",
    "        #print(out.shape)\n",
    "        out = out.contiguous().view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.softmax(out)\n",
    "        # reshape to be batch_size first\n",
    "        #print(out.shape)\n",
    "        #out = out.view(batch_size,n_cell, -1)\n",
    "        #print(out.shape)\n",
    "        #out = out[:, -1] # get last batch of labels\n",
    "        #print(out.shape)\n",
    "        # return last sigmoid output and hidden state\n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size, train_on_gpu = False):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().float())\n",
    "        \n",
    "        return hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
